{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherry-tang-97/bag_of_words/blob/main/bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-2yajRihrbk",
        "outputId": "88b6ca55-380f-48ee-fb92-2a8390c5a2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5pI7QeCz0RS"
      },
      "outputs": [],
      "source": [
        "#Importing packages\n",
        "import pandas as pd       \n",
        "from bs4 import BeautifulSoup  \n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from gensim.models import word2vec\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODQjWn074Am8",
        "outputId": "40c8d0a7-5454-463e-fd96-7535429a3971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwICyR4lveqA",
        "outputId": "898847d4-c1d7-442e-d1bf-043d9537f0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 3)\n",
            "(25000, 2)\n",
            "(50000, 2)\n"
          ]
        }
      ],
      "source": [
        "# Read in the data\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/bag_of_words_data/labeledTrainData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/bag_of_words_data/testData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "unlabeled_train = pd.read_csv( \"/content/drive/MyDrive/bag_of_words_data/unlabeledTrainData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(unlabeled_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWBhsQFMztQz"
      },
      "outputs": [],
      "source": [
        "#Data cleaning\n",
        "def review_to_words( raw_review ):\n",
        "    # Function to convert a raw review to a string of words\n",
        "    # The input is a single string (a raw movie review), and \n",
        "    # the output is a single string (a preprocessed movie review)\n",
        "    #\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(raw_review).get_text() \n",
        "    #\n",
        "    # 2. Remove non-letters        \n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
        "    #\n",
        "    # 3. Convert to lower case, split into individual words\n",
        "    words = letters_only.lower().split()                             \n",
        "    #\n",
        "    # 4. In Python, searching a set is much faster than searching\n",
        "    #   a list, so convert the stop words to a set\n",
        "    stops = set(stopwords.words(\"english\"))                  \n",
        "    # \n",
        "    # 5. Remove stop words\n",
        "    meaningful_words = [w for w in words if not w in stops]   \n",
        "    #\n",
        "    # 6. Join the words back into one string separated by space, \n",
        "    # and return the result.\n",
        "    return( \" \".join( meaningful_words ))\n",
        "\n",
        "\n",
        "\n",
        "#Clean train reviews\n",
        "\n",
        "num_reviews_train = train[\"review\"].size\n",
        "clean_train_reviews = []\n",
        "for i in range(0, num_reviews_train):\n",
        "    clean_train_reviews.append(review_to_words(train[\"review\"][i]))\n",
        "\n",
        "\n",
        "#Clean test reviews\n",
        "\n",
        "num_reviews_test = test[\"review\"].size\n",
        "clean_test_reviews = []\n",
        "for i in range(0, num_reviews_test):\n",
        "    clean_test_reviews.append(review_to_words(test[\"review\"][i]))\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIdK3zsnk3__",
        "outputId": "d1f9bc30-860c-410e-83d9-3844cd6c9fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 5000)\n",
            "(25000, 5000)\n"
          ]
        }
      ],
      "source": [
        "#Feature Engineer\n",
        "#Mehotd 1: bag of words\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000) \n",
        "\n",
        "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
        "train_data_features = train_data_features.toarray()\n",
        "\n",
        "test_data_features = vectorizer.transform(clean_test_reviews)\n",
        "test_data_features = test_data_features.toarray()\n",
        "\n",
        "print(train_data_features.shape)\n",
        "print(test_data_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZS5zOkIqgH8"
      },
      "outputs": [],
      "source": [
        "#Random Forest Model\n",
        "forest = RandomForestClassifier(n_estimators = 100) \n",
        "forest = forest.fit(train_data_features, train[\"sentiment\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkqY23YiraT6",
        "outputId": "bb8aa172-972c-4999-9ab5-ef2f0a5e9c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 ... 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "#Prediction\n",
        "result = forest.predict(test_data_features)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivotmX_RryPf"
      },
      "outputs": [],
      "source": [
        "#Using word2vec for feature engineering\n",
        "\n",
        "#Data cleaning\n",
        "def review_to_wordlist( review, remove_stopwords=False ):\n",
        "    # Function to convert a document to a sequence of words,\n",
        "    # optionally removing stop words.  Returns a list of words.\n",
        "    #\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "    #  \n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    #\n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    #\n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    #\n",
        "    # 5. Return a list of words\n",
        "    return(words)\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "    # Function to split a review into parsed sentences. Returns a \n",
        "    # list of sentences, where each sentence is a list of words\n",
        "    #\n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    #\n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
        "              remove_stopwords ))\n",
        "    #\n",
        "    # Return the list of sentences (each sentence is a list of words,\n",
        "    # so this returns a list of lists\n",
        "    return sentences\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEqECVsjJSyw",
        "outputId": "6239e4e5-e3b2-41d6-987a-01bf3aad758d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:332: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:332: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:332: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:332: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:270: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:332: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:332: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "for review in train[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n",
        "\n",
        "for review in unlabeled_train[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VsMXwKhzT4H2"
      },
      "outputs": [],
      "source": [
        "#Train word2vec\n",
        "num_features = 300    # Word vector dimensionality                      \n",
        "min_word_count = 40   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9yLzMFC--YkY"
      },
      "outputs": [],
      "source": [
        "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
        "            size=num_features, min_count = min_word_count, \\\n",
        "            window = context, sample = downsampling)\n",
        "model.init_sims(replace=True)\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY4BI65erHsg"
      },
      "outputs": [],
      "source": [
        "model.most_similar(\"man\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdtiF8n07XIR"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load(\"300features_40minwords_10context\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCqXqLugCrhx"
      },
      "outputs": [],
      "source": [
        "ori_wv=model[model.wv.vocab]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9pVRTGx1yOt"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "std_wv=StandardScaler().fit_transform(ori_wv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6fAcdPT2ObZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pc=PCA(n_components=2).fit_transform(std_wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYBhIJoU3kby"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        " \n",
        "kmeans = KMeans(n_clusters= 1000)\n",
        "label = kmeans.fit_predict(pc)\n",
        " \n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgsSWEAo7BgK"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(pc, columns = ['PC1', 'PC2'])\n",
        "df['cluster']=label\n",
        "df['vocab']=list(model.wv.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B6ZpWgv7pPX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df.plot.scatter(x='PC1', y='PC2', c='cluster', colormap='viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFd_iTsRErcr"
      },
      "outputs": [],
      "source": [
        "df1=df[df.cluster==6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAiuDHFF95vw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x=list(df1.PC1)\n",
        "y=list(df1.PC2)\n",
        "l=list(df1.vocab)\n",
        "\n",
        "\n",
        "plt.scatter(x, y)\n",
        "\n",
        "for i, txt in enumerate(l):\n",
        "    plt.annotate(txt, (x[i], y[i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae32MECWIJB4"
      },
      "outputs": [],
      "source": [
        "model.most_similar('great')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZcrqd_MJpvS"
      },
      "outputs": [],
      "source": [
        "syn=[i[0] for i in model.most_similar('great')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRIAEO14KrvC"
      },
      "outputs": [],
      "source": [
        "syn_good=[i[0] for i in model.most_similar('good')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRDBUpUMK-t5"
      },
      "outputs": [],
      "source": [
        "all=syn+syn_good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKZbVbqXI561"
      },
      "outputs": [],
      "source": [
        "plot=df[df.vocab.isin(all)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSLoeDCPKiKB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x=list(plot.PC1)\n",
        "y=list(plot.PC2)\n",
        "l=list(plot.vocab)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "\n",
        "for i, txt in enumerate(l):\n",
        "    plt.annotate(txt, (x[i], y[i]))\n",
        "\n",
        "\n",
        "plt.title('Word Embedding Space')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OuzR2bcOt0v"
      },
      "outputs": [],
      "source": [
        "#Feature Engineering\n",
        "#Method 1: vector averaging\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    # Function to average all of the word vectors in a given\n",
        "    # paragraph\n",
        "    #\n",
        "    # Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    #\n",
        "    nwords = 0\n",
        "    # \n",
        "    # Index2word is a list that contains the names of the words in \n",
        "    # the model's vocabulary. Convert it to a set, for speed \n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    #\n",
        "    # Loop over each word in the review and, if it is in the model's\n",
        "    # vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set: \n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    # \n",
        "    # Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n",
        "\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    # Given a set of reviews (each one a list of words), calculate \n",
        "    # the average feature vector for each one and return a 2D numpy array \n",
        "    # \n",
        "    # Initialize a counter\n",
        "    counter = 0\n",
        "    # \n",
        "    # Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    # \n",
        "    # Loop through the reviews\n",
        "    for review in reviews:\n",
        "       #\n",
        "       # Print a status message every 1000th review\n",
        "       if counter%1000 == 0:\n",
        "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
        "       # \n",
        "       # Call the function (defined above) that makes average feature vectors\n",
        "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
        "           num_features)\n",
        "       #\n",
        "       # Increment the counter\n",
        "       counter = counter + 1\n",
        "    return reviewFeatureVecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLp3Zu7wO9TZ"
      },
      "outputs": [],
      "source": [
        "clean_train_reviews = []\n",
        "for review in train[\"review\"]:\n",
        "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
        "        remove_stopwords=True ))\n",
        "\n",
        "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
        "\n",
        "print (\"Creating average feature vecs for test reviews\")\n",
        "clean_test_reviews = []\n",
        "for review in test[\"review\"]:\n",
        "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
        "        remove_stopwords=True ))\n",
        "\n",
        "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JoTy8ImPC-w"
      },
      "outputs": [],
      "source": [
        "#Fit a random forest\n",
        "forest = RandomForestClassifier( n_estimators = 100 )\n",
        "\n",
        "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
        "\n",
        "#Predict\n",
        "result = forest.predict( testDataVecs )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkpbgFGegOP5"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from string import punctuation\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IAoD-Jlp24H"
      },
      "outputs": [],
      "source": [
        "test['sentiment']=result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtmbOlmq48z0"
      },
      "outputs": [],
      "source": [
        "test.sentiment.sum()/25000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibU0Im8XrClc"
      },
      "outputs": [],
      "source": [
        "porportion = [49.81,50.19]\n",
        "  \n",
        "# colors\n",
        "colors = ['green', 'orange']\n",
        "\n",
        "labels=['Positive', 'Negative']\n",
        "# Pie Chart\n",
        "plt.pie(porportion, colors=colors, labels=labels,\n",
        "        autopct='%1.1f%%', pctdistance=0.85)\n",
        "  \n",
        "# draw circle\n",
        "centre_circle = plt.Circle((0, 0), 0.65, fc='white')\n",
        "fig = plt.gcf()\n",
        "  \n",
        "# Adding Circle in Pie chart\n",
        "fig.gca().add_artist(centre_circle)\n",
        "  \n",
        "# Adding Title of chart\n",
        "plt.title('Distribution of Positive vs Negative Reviews')\n",
        "plt.text(0, 0, '25000 Reviews in Total', ha='center', va='center', fontsize=9.5)\n",
        "# Displaying Chart\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rnpwXZTtZCY"
      },
      "outputs": [],
      "source": [
        "neg=test[test.sentiment==0]\n",
        "pos=test[test.sentiment==1]\n",
        "neg=neg.reset_index()\n",
        "pos=pos.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5RfgiybHC4x"
      },
      "outputs": [],
      "source": [
        "def clean_review( review, remove_stopwords=False ):\n",
        "    \n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "    #  \n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    #\n",
        "    # 3. Convert words to lower case\n",
        "    review_text = review_text.lower()\n",
        "    #\n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        review_text = [w for w in review_text if not w in stops]\n",
        "  \n",
        "    # 5. Return a list of words\n",
        "    return(review_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU4K-rj8I8xW"
      },
      "outputs": [],
      "source": [
        "num_pos=pos['review'].size\n",
        "clean_pos=[]\n",
        "for i in range(0, num_pos):\n",
        "  clean_pos.append(clean_review(pos['review'][i]))\n",
        "\n",
        "num_neg=neg['review'].size\n",
        "clean_neg=[]\n",
        "for i in range(0, num_neg):\n",
        "  clean_neg.append(clean_review(neg['review'][i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uOTkLSXlLVj"
      },
      "outputs": [],
      "source": [
        "def extract_keywords(nlp, sequence, pos_tag, special_tags : list = None):\n",
        "    \n",
        "    result = []\n",
        "\n",
        "    # custom list of part of speech tags we are interested in\n",
        "    # we are interested in proper nouns, nouns, and adjectives\n",
        "    # edit this list of POS tags according to your needs. \n",
        "    #pos_tag = ['PROPN','NOUN','ADJ']\n",
        "\n",
        "    # create a spacy doc object by calling the nlp object on the input sequence\n",
        "    doc = nlp(sequence.lower())\n",
        "\n",
        "    # if special tags are given and exist in the input sequence\n",
        "    # add them to results by default\n",
        "    if special_tags:\n",
        "        tags = [tag.lower() for tag in special_tags]\n",
        "        for token in doc:\n",
        "            if token.text in tags:\n",
        "                result.append(token.text)\n",
        "    \n",
        "    for chunk in doc.noun_chunks:\n",
        "        final_chunk = \"\"\n",
        "        for token in chunk:\n",
        "            if (token.pos_ in pos_tag):\n",
        "                final_chunk =  final_chunk + token.text + \" \"\n",
        "        if final_chunk:\n",
        "            result.append(final_chunk.strip())\n",
        "\n",
        "\n",
        "    for token in doc:\n",
        "        if (token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
        "            continue\n",
        "        if (token.pos_ in pos_tag):\n",
        "            result.append(token.text)\n",
        "    return list(set(result))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgoq2TlRzPO8"
      },
      "source": [
        "positive reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeGf_6Bo6bux"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcwI3ZFd69D3"
      },
      "outputs": [],
      "source": [
        "unwanted=['film', 'movie', 'time', 'movies', 'films']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_Yl2iNGvcan"
      },
      "outputs": [],
      "source": [
        "len(clean_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6xchEf5lTiS"
      },
      "outputs": [],
      "source": [
        "keys=''\n",
        "for i in range(1000):\n",
        "  extracted=extract_keywords(nlp, clean_pos[i], pos_tag=['NOUN'])\n",
        "  keys = keys + ' '.join(extracted) + ' '\n",
        "\n",
        "prefilter=keys.split()\n",
        "filtered=[word for word in prefilter if word not in unwanted]\n",
        "final=' '.join(filtered)\n",
        "\n",
        "wordcloud = WordCloud(stopwords = STOPWORDS,\n",
        "                      collocations=True).generate(final)\n",
        "plt.imshow(wordcloud, interpolation='bilInear')\n",
        "plt.title('Keywords for Positive Reviews')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdMoR-Iq50kr"
      },
      "source": [
        "negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arcq2myN6BHe"
      },
      "outputs": [],
      "source": [
        "keys=''\n",
        "for i in range(1000):\n",
        "  extracted=extract_keywords(nlp, clean_neg[i], pos_tag=['NOUN'])\n",
        "  keys = keys + ' '.join(extracted) + ' '\n",
        "\n",
        "prefilter=keys.split()\n",
        "filtered=[word for word in prefilter if word not in unwanted]\n",
        "final=' '.join(filtered)\n",
        "\n",
        "wordcloud = WordCloud(stopwords = STOPWORDS,\n",
        "                      collocations=True).generate(final)\n",
        "plt.imshow(wordcloud, interpolation='bilInear')\n",
        "plt.title('Keywords for Negative Reviews')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5ntXqMA76yg"
      },
      "source": [
        "what about adjective?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4pmaJ_c8EYt"
      },
      "outputs": [],
      "source": [
        "unwanted=['good', 'bad', 'great', 'little', 'young', 'old', 'many', 'best', 'better', 'first', 'worst', 'real']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GqH0jqx75zY"
      },
      "outputs": [],
      "source": [
        "keys=''\n",
        "for i in range(500):\n",
        "  extracted=extract_keywords(nlp, clean_pos[i], pos_tag=['VERB'])\n",
        "  keys = keys + ' '.join(extracted) + ' '\n",
        "\n",
        "prefilter=keys.split()\n",
        "filtered=[word for word in prefilter if word not in unwanted]\n",
        "final=' '.join(filtered)\n",
        "\n",
        "wordcloud = WordCloud(stopwords = STOPWORDS,\n",
        "                      collocations=True).generate(final)\n",
        "plt.imshow(wordcloud, interpolation='bilInear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVxYbHOI9Q8e"
      },
      "outputs": [],
      "source": [
        "keys=''\n",
        "for i in range(2000):\n",
        "  extracted=extract_keywords(nlp, clean_neg[i], pos_tag=['ADJ'])\n",
        "  keys = keys + ' '.join(extracted) + ' '\n",
        "\n",
        "prefilter=keys.split()\n",
        "filtered=[word for word in prefilter if word not in unwanted]\n",
        "final=' '.join(filtered)\n",
        "\n",
        "wordcloud = WordCloud(stopwords = STOPWORDS,\n",
        "                      collocations=True).generate(final)\n",
        "plt.imshow(wordcloud, interpolation='bilInear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAjER69VCWAp"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
        "# average of 5 words per cluster\n",
        "word_vectors = model.wv.syn0\n",
        "num_clusters = int(word_vectors.shape[0] / 5)\n",
        "\n",
        "# Initalize a k-means object and use it to extract centroids\n",
        "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
        "idx = kmeans_clustering.fit_predict( word_vectors )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpCB5WuMJPWC"
      },
      "outputs": [],
      "source": [
        "word_centroid_map = dict(zip(model.wv.index2word, idx))\n",
        "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
        "    #\n",
        "    # The number of clusters is equal to the highest cluster index\n",
        "    # in the word / centroid map\n",
        "    num_centroids = max( word_centroid_map.values() ) + 1\n",
        "    #\n",
        "    # Pre-allocate the bag of centroids vector (for speed)\n",
        "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
        "    #\n",
        "    # Loop over the words in the review. If the word is in the vocabulary,\n",
        "    # find which cluster it belongs to, and increment that cluster count \n",
        "    # by one\n",
        "    for word in wordlist:\n",
        "        if word in word_centroid_map:\n",
        "            index = word_centroid_map[word]\n",
        "            bag_of_centroids[index] += 1\n",
        "    #\n",
        "    # Return the \"bag of centroids\"\n",
        "    return bag_of_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq-TZ_jEJxe5"
      },
      "outputs": [],
      "source": [
        "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
        "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
        "    dtype=\"float32\" )\n",
        "\n",
        "# Transform the training set reviews into bags of centroids\n",
        "counter = 0\n",
        "for review in clean_train_reviews:\n",
        "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
        "        word_centroid_map )\n",
        "    counter += 1\n",
        "\n",
        "# Repeat for test reviews \n",
        "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
        "    dtype=\"float32\" )\n",
        "\n",
        "counter = 0\n",
        "for review in clean_test_reviews:\n",
        "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
        "        word_centroid_map )\n",
        "    counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PHNi2ueVZIF"
      },
      "outputs": [],
      "source": [
        "# Fit a random forest and extract predictions \n",
        "forest = RandomForestClassifier(n_estimators = 100)\n",
        "\n",
        "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
        "result = forest.predict(test_centroids)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}